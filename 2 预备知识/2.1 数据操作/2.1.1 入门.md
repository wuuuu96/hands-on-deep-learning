torch.randn(size) 随机生成均值0，方差1的正态分布(高斯分布)数据，**size=2,一维** **长度为2 size=(2,3),二维 长度为2和3**

torch.rand(size) 随机生成符合[0, 1)均匀分布数据 

torch.randint(low, high, size) 随机生成在范围[low,high)的整数

torch.normal(mean,std,size) 随机生成均值为mean，标准差为std的正态分布(高斯分布)数据

torch.tensor(data, dtype=None, device=None) 直接把 Python 数据转成 Tensor,其中默认 dtype 由数据推断

x.shape/x.size

x.view/x.reshape

x.unsqueeze/x.squeeze

x.permute

x.ndim x.numel()

torch.arange/torch.linspace/

torch.meshgrid

torch.stack/torch.cat

<img width="501" height="177" alt="image" src="https://github.com/user-attachments/assets/8bb152ae-a597-44f1-a97f-62dae18f9f33" />

答案：
```python
x = torch.linspace(-1,1,50)
X,Y = torch.meshgrid(x,x,indexing='ij') #X:(50,50),Y:(50,50)
Z = torch.stack([X,Y],dim=-1) # Z : (50,50,2)
Z = Z.reshape(2500,2) # Z : (2500,2)
Z = Z.unsqueeze(0)
Z = Z.permute(2,1,0)
Z.shape
```
### 1.torch.arange的基本用法 ###

**只能生成一维自然数序列**
**step：步长（默认为 1）**
**所以x.shape=维度size，x.nidm=1**

```python
torch.arange(start=0, end, step=1, dtype=None, device=None)
# 1.step：步长（默认为 1）
# 2.如果没有写start,就是从0开始
# 3.左闭右开
```

**只写 end： 用 torch.arange 生成 0 到 12的一维张量**
```python
x = torch.arange(13)
```

**写 start 和 end：用 torch.arange 生成 2 到 6的一维张量**
```python
tensor([2, 3, 4, 5, 6])
```

**写步长 step：用 torch.arange生成从 1 到 9，步长为 2的一维张量**

```
torch.arange(start=1, end=10, step=2)
```

### 2.torch.linspace的基本用法 ###
**只能生成一维序列**
**step：生成的点数**
**所以x.shape=维度size，x.nidm=1**

```python
torch.linspace(start, end, steps=100, dtype=None, device=None)
# 1.step：生成的点数（默认为 100）
# 2.左闭右闭
```

**生成 0~1 的 5 个等间隔点**
```python
torch.linspace(0, 1, 5) #结果是包括1的
```

**生成 −π 到 π 的 1000 个点，并且只显示前10个点**
```python
x = torch.linspace(-torch.pi, torch.pi, 1000)[:10]
```

### 3.torch.arange和torch.linspace的区别
<img width="901" height="543" alt="image" src="https://github.com/user-attachments/assets/7ae3ffdc-fdc5-40de-9038-5ad26f5b3ce4" />


### 4.张量的现状属性shape   (张量.shape)

在PyTorch 里：```x.shape``` 是 **张量的形状信息**，返回的是一个 **torch.Size 对象**，本质上可以当成**查看张量的维度长度**。并且x.shape   ==   x.size()

**查看三维张量的第1维，第2维，第3维的长度**
```python
import torch
x = torch.randn(2, 3, 4)
print(x.shape)
```
输出：
```python
torch.Size([2, 3, 4])
# 第 0 维：长度 2
# 第 1 维：长度 3 
# 第 2 维：长度 4
# 的2×3×4 的三维数组。
```

**查看一维张量（向量）的维度数和维度长度**
```python
import torch
x = torch.randn(5)
print(x.ndim)
print(x.shape)
```
**查看二维张量（矩阵）的第二维维度长度**
```python
import torch
x = torch.randn(3,4)
print(x.shape[1])
```
**查看四维张量的最后一维的维度长度和倒数第二维的维度长度**
```python
import torch
x = torch.randn(4,1,28,27) #(N, C, H, W)
print(x.shape[-1])
print(x.shape[-2])
```

**x.shape 和 x.size() 的关系**
**在 PyTorch 中：```x.shape   ==   x.size()```几乎完全等价。**
```python
x = torch.randn(2, 3, 4)

print(x.shape)      # torch.Size([2, 3, 4])
print(x.size())     # torch.Size([2, 3, 4])

print(x.size(0))    # 2  第 0 维长度
print(x.size(1))    # 3  第 1 维长度
print(x.size(2))    # 4  第 2 维长度
```

### 5.x.view的用法 ###
**在不改变数据内容的情况下，改变张量形状（shape）**
**其在应用时有一个大前提 ：x 必须是 连续内存 tensor,如果x不是连续内存要用 x.contiguous()将其转换成连续内存**
<img width="683" height="433" alt="image" src="https://github.com/user-attachments/assets/a8f371eb-e4a0-4326-b63b-5d344dbb009a" />

*将三维张量形状改成二维张量现状*
```python
import torch
x = torch.randn(2, 3, 4)
print(x.shape)   # torch.Size([2, 3, 4])

#利用x.view
x = x.view(2, 12)
print(x.shape)      # torch.Size([2, 12])

```

*利用x.view的自动推断展平为二维和一维*
```python

import torch
x = torch.randn(4, 3, 28, 28)
print(x.shape)   # torch.Size([4, 3, 28, 28])

#利用x.view的自动推断展平
x = x.view(4, -1)
print(x.shape)      # torch.Size([4, 2352])


x = torch.randn(2, 3, 4)
y = x.view(-1)
print(y.shape)      # torch.Size([24])


```
*利用x.view的自动推断展平指定维*

```python

import torch
x = torch.randn(8, 16, 7, 7)
x = x.view(x.shape[1], -1) #第二维不变
# 或者
# x = x.view(x.size(1),-1)
x.shape # torch.Size([16, 392])

```

### 和x.reshape的区别：
<img width="967" height="445" alt="image" src="https://github.com/user-attachments/assets/d1bc4108-4ae1-452b-9aee-703c7d8728e8" />

### transpose 后张量不再连续，所以使用x.reshape

### 6.x.reshape的用法 ###

### 改变张量的形状（shape）,不改变数据本身和总元素个数,
### 和 view() 类似，但更智能,和view 的关键区别是reshape()不要求张量是连续内存

*基本用法*
``` python

import torch

x = torch.randn(2, 3, 4)
print(x.shape)          # torch.Size([2, 3, 4])

y = x.reshape(6, 4)
print(y.shape)          # torch.Size([6, 4])

```

*-1 让 PyTorch 自动推算*
*自动推算一维*
``` python

x = torch.randn(2, 3, 4)
y = x.reshape(-1)
print(y.shape)      # torch.Size([24])


```

*自动推算二维*
``` python

x = torch.randn(4, 3, 28, 28)
y = x.reshape(4, -1)
print(y.shape)      # torch.Size([4, 2352])

```

* 指定维度不写死（最推荐）*
``` python

x = torch.randn(8, 16, 7, 7)
y = x.reshape(x.shape[0], -1)

print(y.shape)      # torch.Size([8, 784])

```

### 7.x.unsqueeze的用法 ###

x.unsqueeze(dim) 是 PyTorch 里**加一个新维度并且新维度长度恒为 1**的操作

* 指定维度不写死（最推荐）*
``` python

import torch
x = torch.randn(3)
print(x.shape)   #torch.Size([3])

# 在第 0 维加一维长度为1的维度
y = x.unsqueeze(0)
print(y.shape)   #torch.Size([1,3])

# 在第 1 维加一维长度为1的维度
z = x.unsqueeze(1)
print(z.shape)   #torch.Size([3,1])


```

### 8.x.squeeze的用法 ###

x.squeeze() 是**删除所有 size(长度) 为 1 的维度**
x.squeeze(dim) 是 PyTorch 里**删除维度为1**的操作

*删除所有 size(长度) 为 1 的维度*
``` python

import torch
x = torch.randn(1, 3, 1, 4)
print(x.shape) # torch.Size([1, 3, 1, 4])

y = x.squeeze()
print(y.shape) # torch.Size([3, 4])

```

*删除维度为1的维度，如果维度的size不为1，则没有变化*

``` python
x = torch.randn(2, 3, 1)

x.squeeze(0).shape   # 第0维是2，不是1 → 不变
# torch.Size([2, 3, 1])

x.squeeze(2).shape   # 第2维是1 → 被去掉
# torch.Size([2, 3])

```
### 9.x.permute的用法 ###

permute 是**用0,1,2,3,...重新排列维度顺序**

*(2,3,4) -> (4,3,2)*

``` python
x = torch.randn(2,3,4)
x.shape   # torch.Size([2, 3, 4])

x = x.permute(2,1,0)
x.shape   # torch.Size([4, 3, 2])

```
### 10.torch.meshgrid的用法 ###

根据**多个一维张量**生成**网格坐标矩阵**，是构造笛卡尔坐标网格最常用的函数

```python
torch.meshgrid(*tensors, indexing='ij')

# *tensors：多个 1D 张量（常见 2 个或 3 个）
# indexing
  #'ij'（默认）矩阵索引，适合数学/张量
  #'xy' 笛卡尔索引，适合图像坐标（与 numpy 一致）
```
### 11.torch.stack的用法 ###

**可以指定维度来叠张量个数**
**在一个新的维度上拼接多个张量,所有维度必须完全一致**
**stack = 先扩维，再拼接（所有张量形状必须完全一致）,dim 决定新轴插在第几维，会有新的维度产生**

```python
torch.stack(tensors, dim=0)
# tensors：张量列表，如 [t1, t2, t3...]
# dim：新维度插入的位置（默认 0 维）
```

```python
import torch

a = torch.tensor([1,2,3])
b = torch.tensor([4,5,6])

c = torch.stack([a,b],dim=0) # dim 决定新轴插在第几维
print(c)
print(c.ndim)
print(c.shape) 
  #  tensor([[1, 2, 3],
  #          [4, 5, 6]])
  #  2
  #  torch.Size([2, 3])

```

```python
import torch

a = torch.tensor([1,2,3])
b = torch.tensor([4,5,6])

c = torch.stack([a,b],dim=1) # dim 决定新轴插在第几维
print(c)
print(c.ndim)
print(c.shape) 
  #  tensor([[1, 4],
  #          [2, 5],
  #          [3, 6]])
  #  2
  #  torch.Size([3, 2])

```

和torch.cat的区别 **cat = 平铺 , stack = 叠层**
``` python

a = torch.tensor([1,2,3])
b = torch.tensor([4,5,6])

torch.stack([a,b]).shape   # → (2,3)
torch.cat([a,b]).shape     # → (6,)

```
### 11.torch.cat的用法 ###

**是在“已有维度”上拼接，不会增加新维度。**
```python
torch.cat(tensors, dim=0)
# tensors：张量列表，如 [t1, t2, t3...]
# dim：在哪一个维度上进行拼接,要求：除拼接维外，其余维度必须严格相同
```

*一维张量的cat拼接*
```python
a = torch.tensor([[2,3,4]])  # tensor[2,3,4] torch.Size([1,3]) 1
b = torch.tensor([[5,6,7]])

c = torch.cat([a,b],0)
# tensor([1, 2, 3, 4, 5, 6]) torch.Size([6]) 1

```

*在行方向拼接(立起来)(dim=0),拼接维度可以不同，其他维度必须相同*
```python
a = torch.tensor([[1,2,3],[4,5,6]]) # tensor([[1, 2, 3],
                                             [4, 5, 6]])  torch.Size([2, 3]) 2
b = torch.tensor([[7,8,9]])

c = torch.cat([a,b],dim=0)

tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]]) torch.Size([3, 3]) 3

```

*在列方向拼接(躺下去)(dim=1),拼接维度可以不同，其他维度必须相同*
```python
a = torch.tensor([[1,2,3],[4,5,6]]) # tensor([[1, 2, 3],
                                             [4, 5, 6]])  torch.Size([2, 3]) 2
b = torch.tensor([[7,8],[9,10]]) # torch.Size([2, 2]) 2

c = torch.cat([a,b],dim=1)

tensor([[ 1,  2,  3,  7,  8],
        [ 4,  5,  6,  9, 10]]) torch.Size([2, 5]) 2

```


